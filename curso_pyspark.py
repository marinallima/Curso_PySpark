# -*- coding: utf-8 -*-
"""Curso_PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HYjtcrcd0zKtDd67_YAxzdGbileHLRBZ

# Começando o Trabalho
---

## Apache Spark - Introdução

### [Apache Spark](https://spark.apache.org/)

Apache Spark é uma plataforma de computação em *cluster* que fornece uma API para programação distribuída para processamento de dados em larga escala, semelhante ao modelo *MapReduce*, mas projetada para ser rápida para consultas interativas e algoritmos iterativos.

O Spark permite que você distribua dados e tarefas em clusters com vários nós. Imagine cada nó como um computador separado. A divisão dos dados torna mais fácil o trabalho com conjuntos de dados muito grandes porque cada nó funciona processa apenas uma parte parte do volume total de dados.

O Spark é amplamente utilizado em projetos analíticos nas seguintes frentes:

- Preparação de dados
- Modelos de machine learning
- Análise de dados em tempo real

### [PySpark](https://spark.apache.org/docs/3.1.2/api/python/index.html)

PySpark é uma interface para Apache Spark em Python. Ele não apenas permite que você escreva aplicativos Spark usando APIs Python, mas também fornece o *shell* PySpark para analisar interativamente seus dados em um ambiente distribuído. O PySpark oferece suporte à maioria dos recursos do Spark, como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) e Spark Core.

<center><img src="https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/img-001.png"/></center>

#### Spark SQL e DataFrame

Spark SQL é um módulo Spark para processamento de dados estruturados. Ele fornece uma abstração de programação chamada DataFrame e também pode atuar como mecanismo de consulta SQL distribuído.

#### Spark Streaming

Executando em cima do Spark, o recurso de *streaming* no Apache Spark possibilita o uso de poderosas aplicações interativas e analíticas em *streaming* e dados históricos, enquanto herda a facilidade de uso do Spark e as características de tolerância a falhas.

#### Spark MLlib

Construído sobre o Spark, MLlib é uma biblioteca de aprendizado de máquina escalonável que fornece um conjunto uniforme de APIs de alto nível que ajudam os usuários a criar e ajustar *pipelines* de aprendizado de máquina práticos.

#### Spark Core

Spark Core é o mecanismo de execução geral subjacente para a plataforma Spark sobre o qual todas as outras funcionalidades são construídas. Ele fornece um RDD (*Resilient Distributed Dataset*) e recursos de computação na memória.

## Utilizando o Spark no Google Colab

Para facilitar o desenvolvimento de nosso projeto neste curso vamos utilizar o Google Colab como ferramenta e para configurar o PySpark basta executar os comandos abaixo na própria célula do seu *notebook*.
"""

# instalar as dependências
!apt-get update -qq
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz
!tar xf spark-3.1.2-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

import findspark
findspark.init()

"""# Carregamento de Dados
---

## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)

O ponto de entrada para programar o Spark com a API Dataset e DataFrame.

Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrão de construtor:
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master('local[*]') \
    .appName("Iniciando com Spark") \
    .getOrCreate()

spark

"""## Acessando o [Spark UI](https://spark.apache.org/docs/3.1.2/web-ui.html) (Google Colab)"""

from pyspark.sql import SparkSession

spark = SparkSession.builder\
     .master('local[*]')\
    .appName("Iniciando com Spark")\
    .config('spark.ui.port', '4040')\
    .getOrCreate()

"""## DataFrames com Spark


### Interfaces Spark

Existem três interfaces principais do Apache Spark que você deve conhecer: Resilient Distributed Dataset, DataFrame e Dataset.

- **Resilient Distributed Dataset**: A primeira abstração do Apache Spark foi o Resilient Distributed Dataset (RDD). É uma interface para uma sequência de objetos de dados que consiste em um ou mais tipos localizados em uma coleção de máquinas (um cluster). Os RDDs podem ser criados de várias maneiras e são a API de “nível mais baixo” disponível. Embora esta seja a estrutura de dados original do Apache Spark, você deve se concentrar na API DataFrame, que é um superconjunto da funcionalidade RDD. A API RDD está disponível nas linguagens Java, Python e Scala.

- **DataFrame**: Trata-se de um conceito similar ao DataFrame que você pode estar familiarizado como o pacote pandas do Python e a linguagem R . A API DataFrame está disponível nas linguagens Java, Python, R e Scala.

- **Dataset**: uma combinação de DataFrame e RDD. Ele fornece a interface digitada que está disponível em RDDs enquanto fornece a conveniência do DataFrame. A API Dataset está disponível nas linguagens Java e Scala.

Em muitos cenários, especialmente com as otimizações de desempenho incorporadas em DataFrames e Datasets, não será necessário trabalhar com RDDs. Mas é importante entender a abstração RDD porque:

- O RDD é a infraestrutura subjacente que permite que o Spark seja executado com tanta rapidez e forneça a linhagem de dados.

- Se você estiver mergulhando em componentes mais avançados do Spark, pode ser necessário usar RDDs.

- As visualizações na Spark UI fazem referência a RDDs.
"""

data = [('Zeca','35'), ('Eva', '29')]
colNames = ['Nome', 'Idade']
df = spark.createDataFrame(data, colNames)
df

df.show()

df.toPandas()

"""## Projeto

Nosso projeto consiste em ler, manipular, tratar e salvar um conjunto de dados volumosos utilizando como ferramenta o Spark.

## Carregamento de dados

### Dados Públicos CNPJ
#### Receita Federal

> [Empresas](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)
> 
> [Estabelecimentos](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)
> 
> [Sócios](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)

[Fonte original dos dados](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj)

---
[property SparkSession.read](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html)

[DataFrameReader.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html)

### Montando nosso drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Carregando os dados das empresas"""

path = '/content/drive/MyDrive/Curso_Spark/empresas'
empresas = spark.read.csv(path, sep = ';', inferSchema=True)

empresas.count()

"""## Faça como eu fiz: Estabelecimentos e Sócios

### Carregando os dados dos estabelecimentos
"""

path = '/content/drive/MyDrive/Curso_Spark/estabelecimentos'
estabelecimentos = spark.read.csv(path, sep = ';', inferSchema=True)

estabelecimentos.count()

"""### Carregando os dados dos sócios"""

path = '/content/drive/MyDrive/Curso_Spark/socios'
socios = spark.read.csv(path, sep = ';', inferSchema=True)

socios.count()

"""# Manipulando os Dados
---

## Operações básicas
"""

empresas.limit(5).toPandas()

"""### Renomeando as colunas do DataFrame"""

empresasColNames = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']

for index,colName in enumerate(empresasColNames):
        empresas = empresas.withColumnRenamed(f"_c{index}", colName)

empresas.columns

empresas.limit(5).toPandas()

estabsColNames = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']

for index,colName in enumerate(estabsColNames):
        estabelecimentos = estabelecimentos.withColumnRenamed(f"_c{index}", colName)

estabelecimentos.columns

estabelecimentos.limit(5).toPandas()

sociosColNames = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']

for index,colName in enumerate(sociosColNames):
       socios = socios.withColumnRenamed(f"_c{index}", colName)

socios.columns

socios.limit(5).toPandas()

"""## Analisando os dados

[Data Types](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#data-types)
"""

empresas.limit(5).toPandas()

empresas.printSchema()

socios.limit(5).toPandas()

socios.printSchema()

estabelecimentos.limit(5).toPandas()

estabelecimentos.printSchema()

"""## Modificando os tipos de dados

[Functions](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#functions)

[withColumn](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)

### Convertendo String ➔ Double

#### `StringType ➔ DoubleType`
"""

from pyspark.sql.types import DoubleType, StringType
from pyspark.sql import functions as f

empresas.printSchema()

empresas.limit(5).toPandas()

empresas = empresas.withColumn('capital_social_da_empresa', f.regexp_replace('capital_social_da_empresa', ',', '.'))

empresas.limit(5).toPandas()

empresas.printSchema()

empresas = empresas.withColumn('capital_social_da_empresa', empresas['capital_social_da_empresa'].cast(DoubleType()))

empresas.limit(5).toPandas()

empresas.printSchema()

"""### Convertendo String ➔ Date

#### `StringType ➔ DateType`

[Datetime Patterns](https://spark.apache.org/docs/3.1.2/sql-ref-datetime-pattern.html)
"""

df = spark.createDataFrame([(20200924,), (20201022,), (20210215,)], ['data'])
df.toPandas()

df.printSchema()

"""Primeiro precisa converter pra String to date"""

df = df.withColumn("data", f.to_date(df.data.cast(StringType()), 'yyyyMMdd'))
df.printSchema()

df.toPandas()

estabelecimentos.limit(5).toPandas()

estabelecimentos.printSchema()

estabelecimentos = estabelecimentos\
            .withColumn(
            "data_situacao_cadastral",
            f.to_date(estabelecimentos.data_situacao_cadastral.cast(StringType()), 'yyyyMMdd')
            )\
            .withColumn(
            "data_de_inicio_atividade",
            f.to_date(estabelecimentos.data_de_inicio_atividade.cast(StringType()), 'yyyyMMdd')
            )\
            .withColumn(
            "data_da_situacao_especial",
            f.to_date(estabelecimentos.data_da_situacao_especial.cast(StringType()), 'yyyyMMdd')
            )

estabelecimentos.printSchema()

estabelecimentos.limit(5).toPandas()

socios.printSchema()

socios = socios\
            .withColumn(
            "data_de_entrada_sociedade",
            f.to_date(socios.data_de_entrada_sociedade.cast(StringType()), 'yyyyMMdd')
            )

socios.printSchema()

"""# Seleções e consultas
---

## Selecionando informações
 
[DataFrame.select(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.select.html)
"""

empresas\
        .select('*')\
        .show(5, False)

empresas\
        .select('natureza_juridica', 'porte_da_empresa', 'capital_social_da_empresa')\
        .show(5)

socios\
        .select('nome_do_socio_ou_razao_social', 'faixa_etaria',  f.year('data_de_entrada_sociedade').alias('ano_de_entrada'))\
        .show(5, False)

"""## Faça como eu fiz"""

estabelecimentos.select('nome_fantasia', 'municipio')

estabelecimentos \
    .select('nome_fantasia', 'municipio', 
            f.year('data_de_inicio_atividade').alias('ano_de_inicio_atividade'),
            f.month('data_de_inicio_atividade').alias('mes_de_inicio_atividade')) \
    .show(5, False)

"""## Identificando valores nulos

(1,) > a vírgula é pra ele entender como coluna
"""

df = spark.createDataFrame([(1,), (2,), (3,), (None,)], ['data'])
df.toPandas()

df.show()

"""(1.) > significa que eu quero 1.0"""

df = spark.createDataFrame([(1.,), (2.,), (3.,), (float('nan'),)], ['data'])
df.toPandas()

df.show()

df = spark.createDataFrame([('1',), ('2',), ('3',), (None,)], ['data'])
df.toPandas()

df.show()

socios.limit(5).toPandas()

socios.limit(5).show()

"""Há métodos que nos permitem verificar quantos null's e NaN's existem em determinada coluna, são eles isnull e isnan, respectivamente. Caso seja do seu interesse, leia a documentação.

Outra maneira de fazermos essa contagem é utilizando o método .count(). Para isso, começamos chamando nosso dataframe socios e o método .select(), que nos permite selecionar algo.

Dentro desta lista, chamaremos a função contadora f.count() e o método .alias(), que receberá o parâmetro c, informando que este será o nome usado para nos referirmos a cada coluna.
"""

socios.select([f.count(f.when(f.isnull(c), 1)).alias(c) for c in socios.columns]).show()

socios.limit(5).toPandas()

socios.printSchema()

"""Caso quiséssemos substituir os valores nulos por outros poderíamos utilizar o método fill(). Para isso, chamamos socios.na.fill(), onde na representa um valor nulo, e passamos 0 como parâmetro. Este comando ordena que os valores nulos, ou seja na, sejam substituídos por 0. Em seguida, adicionamos o comando .limit(5).toPandas() a fim de visualizar essa alteração."""

socios.na.fill(0).limit(5).toPandas()

"""Perceba que na coluna "país" todos os dados foram substituídos por 0. No entanto, em "nome do representante" essa alteração não aconteceu, porque os dados desta coluna são do tipo string e o código está considerando somente variáveis do tipo numéricas.

Se, ao invés de 0, trocássemos por um traço socios.na.fill('-').limit(5).toPandas(), a coluna "país" continuaria como "NaN" e o "nome do representante" seria substituído pelo traço. Sendo assim, sempre que precisarmos substituir um dado, precisaremos, primeiro, nos ater a que tipo de variável este dado corresponde.
"""

socios.na.fill('-').limit(5).toPandas()

"""## Ordenando os dados

[DataFrame.orderBy(*cols, **kwargs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)

Agora vamos acrescentar um comando que nos permite ordenar esses dados de forma crescente ou decrescente. Para isso, após select(), adicionamos o método .orderBy(), que receberá como parâmetro a coluna pela qual queremos fazer a ordenação da saída. Nossa intenção é que os anos sejam ordenados, portanto passaremos a coluna 'ano_de_entrada'.
Adicionaremos, ainda ao método .orderBy(), o argumento ascending=False, informando que esta ordem deve ser decrescente, do maior para o menor
"""

socios\
            .select('nome_do_socio_ou_razao_social', 'faixa_etaria', f.year('data_de_entrada_sociedade').alias('ano_de_entrada'))\
            .orderBy('ano_de_entrada',ascending=False)\
            .show(5, False)

"""Agora faremos esse mesmo procedimento para mais de uma coluna. Para isso, as colunas a serem ordenadas pelo método .orderBy() devem ser passadas como listas []. Escolheremos "ano_de_entrada" e "faixa_etaria"."""

socios\
            .select('nome_do_socio_ou_razao_social', 'faixa_etaria', f.year('data_de_entrada_sociedade').alias('ano_de_entrada'))\
            .orderBy(['ano_de_entrada', 'faixa_etaria'], ascending=[False, False])\
            .show(5, False)

"""## Filtrando os dados

[DataFrame.where(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.where.html) ou [DataFrame.filter(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.filter.html)

Utilizaremos os métodos where() e filter(), que também são comandos do SQL e funcionam semelhantemente. Caso queira, clique nos links para consultar a documentação de where e filter. Estes links também estão disponíveis no notebook, na seção "Ordenando os dados".

O método filter() nos permite filtrar linhas usando condicionais, enquanto where() funciona como um alias para o filter, ou seja, ao chamarmos um, também estaremos chamando o outro.

Utilizaremos o dataframe de empresas para entender o funcionamento desses métodos. Nossa intenção é filtrar o capital social de empresas - valor monetário - em torno de R$ 50.

Para isso, vamos chamar nosso dataframe empresas e o método where() - também poderíamos utilizar o filter(). Este método receberá a query (consulta a um banco de dados) que queremos fazer "capital_social_da_empresa e o valor que queremos filtrar ==50" - observe que os argumentos são passado juntos, semelhante a uma string. 

Por fim, adicionamos o método de visualização .show(5, False) - lembre-se que o argumento 5 corresponde aos 5 primeiros registros e False é para que os nomes não venham truncados.
"""

empresas\
            .where("capital_social_da_empresa==50")\
            .show(5, False)

socios\
        .select("nome_do_socio_ou_razao_social")\
        .filter(socios.nome_do_socio_ou_razao_social.startswith("RODRIGO"))\
        .filter(socios.nome_do_socio_ou_razao_social.endswith("DIAS"))\
        .limit(10)\
        .toPandas()

"""## O comando LIKE

[Column.like(other)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.like.html)

Para fecharmos essa seção de seleções e consultas, vamos falar do método like(), que é bem interessante e age como um complemento aos filtros que vimos no vídeo anterior.

Preparamos um dataframe de três linhas, chamado df, que facilitará os nossos testes, criado a partir do spark.createDataFrame().

Dentro da função, em uma coluna chamada data, adicionamos nomes de restaurantes: RESTAURANTE DO RUI, Juca restaurantes ltda e Joca Restaurante.
"""

df = spark.createDataFrame([('RESTAURANTE DO RUI',), ('Juca restaurantes ltda',), ('Joca Restaurante',)], ['data'])
df.toPandas()

"""Vamos supor que desejamos selecionar, em nossa base, os registros de empresas que tenham a palavra "restaurante" em sua razão social. Ou seja, queremos procurar uma característica dentro de uma string.

Para buscarmos os restaurantes nessa base, chamaremos o df\ seguido do método where() (ou filter()), passando uma função. Começaremos com um f, que faz referência ao functions do Spark, seguido da chamada de upper(), função que transforma todos os caracteres minúsculos de uma string em maiúsculos.

%RESTAURANTE% > tem a palavra RESTAURANTE

RESTAURANTE% > começa com a palavra RESTAURANTE

%RESTAURANTE > termina com a palavra RESTAURANTE
"""

df\
    .where(f.upper(df.data).like('%RESTAURANTE%'))\
    .show(truncate=False)

"""E se quisermos fazer isso em nosso dataframe empresas? Começaremos com uma seleção pré-configurada, onde faremos um select() de 'razao_social_nome_empresarial', 'natureza_juridica', 'porte_da_empresa' e 'capital_social_da_empresa'. Logo abaixo, passaremos um filter(), transformando os caracteres em maiúsculos com o método upper() - dessa vez aplicado a empresas['razao_social_nome_empresarial']-, e pegando as ocorrências de RESTAURANTE com o método like().

Finalizaremos com show(15, False), mostrando os 15 primeiros resultados.
"""

empresas\
    .select('razao_social_nome_empresarial', 'natureza_juridica', 'porte_da_empresa', 'capital_social_da_empresa')\
    .filter(f.upper(empresas['razao_social_nome_empresarial']).like('%RESTAURANTE%'))\
    .show(15, False)

"""# Agregações e Junções
---

[DataFrame.groupBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)

[DataFrame.agg(*exprs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.agg.html)

[DataFrame.summary(*statistics)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html)

> Funções:
[approx_count_distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html) | 
[avg](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.avg.html) | 
[collect_list](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_list.html) | 
[collect_set](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_set.html) | 
[countDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.countDistinct.html) | 
[count](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.count.html) | 
[grouping](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.grouping.html) | 
[first](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.first.html) | 
[last](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.last.html) | 
[kurtosis](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.kurtosis.html) | 
[max](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.max.html) | 
[min](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.min.html) | 
[mean](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.mean.html) | 
[skewness](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.skewness.html) | 
[stddev ou stddev_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev.html) | 
[stddev_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev_pop.html) | 
[sum](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sum.html) | 
[sumDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sumDistinct.html) | 
[variance ou var_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.variance.html) | 
[var_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.var_pop.html)

## Sumarizando os dados

Agora faremos um agrupamento dessas informações segundo o ano_de_entrada usando o método groupBy(). Assim, todas as estatísticas ou sumarizações serão feitas sobre essa variável de agrupamento.

Com isso, podemos usar diversas funções, como contagem (count()), média (avg()), dentre outras, e elas serão aplicadas ao agrupamento que criamos. Como exemplo, chamaremos a função count() para obtermos uma contagem dos sócios que entraram a partir do ano de 2010. Além disso, faremos um orderBy() para ordenarmos essas informações de maneira ascendente de acordo com o ano de entrada, e visualizaremos o agrupamento com show().
"""

socios\
    .select(f.year('data_de_entrada_sociedade').alias('ano_de_entrada'))\
    .where('ano_de_entrada >= 2010')\
    .groupBy('ano_de_entrada')\
    .count()\
    .orderBy('ano_de_entrada', ascending=True)\
    .show()

"""Para o próximo exemplo, usaremos o dataframe empresas. Faremos um select() de um conjunto de colunas: cnpj_basico, porte_da_empresa e capital_social_da_empresa. Em seguida, faremos um groupBy() segundo o porte_da_empresa - uma variável numérica com uma classificação que está no dicionário.

Depois, rodaremos a função agg(), de aggregate. Ou seja, faremos uma agregação, criando mais de um conjunto de estatísticas dentro do groupBy() que executamos.

Dentro dela, chamaremos a função avg() (que também poderia ser a função mean(), com o mesmo resultado) para obtermos a média do capital_social_da_empresa, resultado que chamaremos de capital_social_media. Outra informação que queremos obter é uma contagem (count()) de quantas empresas aparecem nessa variável porte_da_empresa, informação que chamaremos de frequencia.

Para finalizar, faremos um orderBy() segundo o porte_da_empresa e exibiremos com o show().
"""

empresas\
    .select('cnpj_basico', 'porte_da_empresa', 'capital_social_da_empresa')\
    .groupBy('porte_da_empresa')\
    .agg(
        f.avg("capital_social_da_empresa").alias("capital_social_medio"),
        f.count("cnpj_basico").alias("frequencia")
    )\
    .orderBy('porte_da_empresa', ascending=True)\
    .show()

"""Como um exemplo final, trabalharemos com outra forma de fazer sumarizações, que é o summary(), uma alternative ao describe(). A partir do dataframe empresas, faremos uma selação do capital_social_da_empresa e chamaremos o método summary(), exibindo os resultado em seguida com o show()."""

empresas\
    .select("capital_social_da_empresa")\
    .summary()\
    .show()

"""## Juntando DataFrames - Joins

[DataFrame.join(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html)

Neste vídeo trataremos das junções de dataframes. Inicialmente faremos essas junções lateralmente, usando uma variável de ligação, e depois apresentaremos um exemplo mais parecido com o append, no qual as informações são colocadas "uma em cima da outra".

Para nos auxiliar nesse processo, temos a função DataFrame.join(). Também preparamos dois dataframes que serão usados como exemplo, produtos e impostos. O primeiro, um dataframe de três colunas, consiste na identificação do produto, uma categoria maior de produtos e o próprio produto em si. O segundo é um dataframe de duas colunas com uma categoria, relacionada com a categoria do dataframe anterior, e a taxa de imposto de cada uma dessas categorias.
"""

produtos = spark.createDataFrame(
    [
        ('1', 'Bebidas', 'Água mineral'), 
        ('2', 'Limpeza', 'Sabão em pó'), 
        ('3', 'Frios', 'Queijo'), 
        ('4', 'Bebidas', 'Refrigerante'),
        ('5', 'Pet', 'Ração para cães')
    ],
    ['id', 'cat', 'prod']
)

impostos = spark.createDataFrame(
    [
        ('Bebidas', 0.15), 
        ('Limpeza', 0.05),
        ('Frios', 0.065),
        ('Carnes', 0.08)
    ],
    ['cat', 'tax']
)

produtos.toPandas()

impostos.toPandas()

"""Repare que as categorias não são iguais em ambos os conjuntos, já que o dataframe impostos possui uma categoria Carnes que não está presente em produtos. Da mesma forma, a categoria Pet presente em produtos não aparece em impostos. Vamos analisar como se dão as junções desses dados e como eles se comportam a depender do método que utilizarmos para essas junções.

Iniciaremos nossa junção chamando o método join() a partir do dataframe produtos, que chamaremos de "dataframe da esquerda". Note que é necessário que ambos os conjuntos possuam uma variável de ligação, nesse caso cat, presente em ambos os dataframes. Dentro da chamada do método, passaremos o segundo dataframe, impostos (que chamaremos de "dataframe da direita"), seguido da variávelde ligação cat e um argumento how.

O argumento how está relacionado a como faremos essa junção, e existem quatro possibilidades principais: inner, left, right e outer. O argumento inner fará a interseção entre os dois conjuntos, e somente nos retornará os valores de cat que existem em ambos os dataframes.

Para o nosso teste, executaremos também o sort(id), uma função parecida com o orderBy(), mas um pouco mais performática mas que não retorna classificações perfeitas, pois atua dentro de particições específicas dos dataframes. Ou seja, se é importante que a classificação esteja correta, o recomendado é utilizar o orderBy()
"""

produtos.join(impostos, 'cat', how='inner')\
    .sort('id')\
    .show()

"""A  construção do método left() é a mesma. Manteremos o conjunto produtos como o dataframe da esquerda e o impostos como o da direita. Quando executamos usando esse argumento, teremos como retorno todas as categorias encontradas no dataframe da esquerda, mantendo aquelas que têm uma inserceção no da direita."""

produtos.join(impostos, 'cat', how='left')\
    .sort('id')\
    .show()

"""Como não temos uma referência para a categoria Pet no dataframe impostos, o valor retornado na coluna tax será null. Além disso, a categoria Carnes, que não existe no dataframe produtos, é omitida.

O argumento right() funciona exatamente da mesma forma, mas retornando apenas as categorias no dataframe da direita, impostos.
"""

produtos.join(impostos, 'cat', how='right')\
    .sort('id')\
    .show()

"""Por fim, temos o argumento outer, que tratá todas as categorias em ambos os conjuntos, retornando null nas colunas sem correspondência."""

produtos.join(impostos, 'cat', how='outer')\
    .sort('id')\
    .show()

empresas.printSchema()

socios.printSchema()

estabelecimentos.printSchema()

"""Rapidamente percebemos que cnpj_basico é uma variável que é comum nelas. Criaremos, então, um dataframe maior chamado empresas_join que receberá a chamada de estabelecimentos.join(), definindo esse conjunto como o dataframe da esquerda. Como argumentos, passaremos o conjunto empresas (que funcionará como dataframe da direita), a coluna cnpj_basico e a propriedade how='inner' para que seja feita a interseção desses conjuntos."""

empresas_join = estabelecimentos.join(empresas, 'cnpj_basico', how='inner')

empresas_join.printSchema()

"""Como retorno, teremos as informações de ambos os dataframes, como porte_da_empresa e capital_social, presentes no conjunto empresas, e nome_fantasia e logradouro, do conjunto estabelecimentos.

Existe também outra maneira de unirmos esses dados, semelhante a um append, de forma que eles fiquem "um sobre o outro". No código a seguir, criaremos uma variável freq que receberá a chamada de um select() a partir do dataframe empresas_join que acabamos de criar.

Nele, pegaremos as empresas pelo cnpj_basico e chamaremos a data_de_incio_atividade pelo apelido data_de_inicio. Em seguida, faremos um filtro para selecionar somente as empresas com data de início igual ou superior a 2010, agruparemos por essa data de início (usando o groupBy()) e faremos uma distribuição de frequência da variável cnpj_basico, ordenando o resultado pela data_de_inicio em ordem ascendente.
"""

freq = empresas_join\
    .select(
        'cnpj_basico', 
        f.year('data_de_inicio_atividade').alias('data_de_inicio')
    )\
    .where('data_de_inicio >= 2010')\
    .groupBy('data_de_inicio')\
    .agg(f.count("cnpj_basico").alias("frequencia"))\
    .orderBy('data_de_inicio', ascending=True)

freq.toPandas()

"""A ideia agora é termos uma somatória da frequência total ao final desse conjunto. Para isso, usaremos o método union(), responsável por juntar dois conjuntos um acima do outro.

Ao executarmos freq.union(), conseguiremos unir todas as informações do conjunto freq com aquelas que passarmos (ou criarmos) dentro da chamada do método. Nesse caso, faremos um freq.select() com duas novas informações: a criação de um rótulo total e a somatória das frequências.

Para criarmos esse rótulo, usaremos f.lit(), uma função que cria um valor literal que chamaremos de Total. Em seguida, chamaremos f.sum() para somarmos toda a coluna freq.frequencia, algo que chamaremos de frequencia.
"""

freq.union(
    freq.select(
        f.lit('Total').alias('data_de_inicio'),
        f.sum(freq.frequencia).alias('frequencia')   
    )
).show()

"""## SparkSQL

[SparkSession.sql(sqlQuery)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.sql.html)

Para saber mais sobre performance: [Artigo - Spark RDDs vs DataFrames vs SparkSQL](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547)

A SparkSession que criamos no começo do treinamento possui um método chamado .sql(). Ele permite que façamos consultas SQL utilizando instruções escritas da maneira padrão, e é isso que aprenderemos a fazer agora. Para que isso seja possível, é necessário que criemos uma view temporária do dataframe, que será usada dentro das instruções SQL em nosso método.

Começaremos chamando o método createOrReplaceTempView() a partir do dataframe empresas. Dentro do método, passaremos o nome da view, que chamaremos de empresasView.
"""

empresas.createOrReplaceTempView("empresasView")

"""Com a view criada, podemos executar spark.sql(). Dentro dos parênteses, passaremos as instruções SQL, no formato de strings. Primeiramente, gostaríamos de fazer um SELECT *, de modo a selecionarmos todas as informações da tabela. Com FROM especificaremos a tabela de onde pegaremos tais informações, que é a nossa empresasView. Por fim, exibiremos os cinco primeiros resultados com show(5)."""

spark.sql("SELECT * FROM empresasView").show(5)

"""A ideia agora é tentarmos montar algumas das seleções que fizemos nas aulas anteriores usando o padrão SQL. Quanto à formatação das instruções SQL, ao invés de passarmos toda a instrução em uma única linha, usando aspas simples, também existe a possibilidade de usar aspas triplas e "pular linhas", facilitando a leitura dessas instruções, como no exemplo abaixo:

Aqui estamos chamando o spark e o método .sql(). Em seguida, utilizamos aspas triplas (no começo e no fim) para criar o nosso ambiente de instrução SQL. Prosseguimos selecionando todas as informações do conjunto (SELECT * FROM empresasview) onde o capital social da empresa seja igual a 50 (WHERE capital_social_da_empresa = 50), uma tabela que já fizemos anteriormente. Por fim, exibiremos os cinco primeiros resultados com show(5).
"""

spark\
    .sql("""
        SELECT * 
            FROM empresasView 
            WHERE capital_social_da_empresa = 50
    """)\
    .show(5)

"""Em nosso próximo exemplo, faremos uma seleção a partir de porte_da_empresa e criaremos uma média do capital_social_da_empresa a partir da instrução MEAN do SQL. Além disso, agruparemos os resultados pelo porte_da_empresa, também usando uma instrução SQL, dessa vez o GROUP BY."""

spark\
    .sql("""
        SELECT porte_da_empresa, MEAN(capital_social_da_empresa) AS Media 
            FROM empresasView 
            GROUP BY porte_da_empresa
    """)\
    .show(5)

"""Anteriormente, criamos um dataframe empresas_join usando o método join() do Spark. Vamos criar uma view desse dataframe, chamada empresasJoinView:"""

empresas_join.createOrReplaceTempView("empresasJoinView")

"""A partir desse conjunto, nós fizemos uma tabela de frequências e a união de dois dataframes diferentes. Vamos aprender a fazer isso com instruções SQL.

Criaremos um novo dataframe freq recebendo a chamada de spark\.sql(). Faremos uma seleção sobre o ano (YEAR) da coluna data_de_inicio_atividade e chamaremos de data_de_inicio. Além disso, usaremos o COUNT para fazer a contagem de cnpj_basico, e chamaremos de count.

Esses dados deverão vir de empresasJoinView, com um filtro onde o ano deverá ser maior ou igual a 2010, agrupados e ordenador pela data_de_inicio.
"""

freq = spark\
    .sql("""
        SELECT YEAR(data_de_inicio_atividade) AS data_de_inicio, COUNT(cnpj_basico) AS count
            FROM empresasJoinView 
            WHERE YEAR(data_de_inicio_atividade) >= 2010
            GROUP BY data_de_inicio
            ORDER BY data_de_inicio
    """)

freq\
    .show()

freq.createOrReplaceTempView("freqView")

"""Anteriormente, para fazermos a união da soma de todas as frequências, usamos o método union(). Com o SQL, faremos uma seleção de todas as informações em freqView e usaremos a instrução UNION ALL. Em seguida, faremos uma seleção passando 'Total' AS data_de_inicio, para criarmos esse campo, e SUM(count) AS count para obtermos a soma das frequências."""

spark\
    .sql("""
        SELECT *
            FROM freqView
        UNION ALL
        SELECT 'Total' AS data_de_inicio, SUM(count) AS count
            FROM freqView
    """)\
    .show()

"""# Formas de Armazenamento
---

## Arquivos CSV

[property DataFrame.write](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.write.html)

[DataFrameWriter.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html)
"""

empresas.write.csv(
    path='/content/drive/MyDrive/Curso_Spark/empresas/csv',
    mode='overwrite',
    sep=';',
    header=True
)

"""Para ler o arquivo:"""

empresas2 = spark.read.csv(
    '/content/drive/MyDrive/Curso_Spark/empresas/csv',
    sep=';',
    inferSchema=True,
    header=True
)

empresas2.printSchema()

"""## Faça como eu fiz"""

socios.write.csv(
    path='/content/drive/MyDrive/Curso_Spark/socios/csv',
    mode='overwrite',
    sep=';',
    header=True
)

socios2 = spark.read.csv(
    '/content/drive/MyDrive/Curso_Spark/socios/csv',
    sep=';',
    inferSchema=True,
    header=True
)

socios2.printSchema()

"""## Arquivos PARQUET

[Apache Parquet](https://parquet.apache.org/)

[DataFrameWriter.parquet(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html)

Neste vídeo conheceremos outro formato de arquivo, o PARQUET. O Apache Parquet é um formato de armazenamento colunar disponível para todos os projetos que pertencem ao ecossistema do Hadoop, como o próprio Spark com o qual estamos trabalhando.

Diferentemente dos modelos "tradicionais" de armazenamento que usam uma abordagem orientada por linhas, o Parquet armazena os dados de forma colunar, o que traz algumas vantagens. Por exemplo, a compressão por colunas é mais eficiente, e as queries que utilizam mais de uma coluna também são mais eficientes.

Arquivos do tipo PARQUET, além de serem menores (mais compactos), são mais performáticos - ou seja, apresentam melhor performance - do que arquivos CSV. Inclusive, após fazer a criação desses arquivos, é interessante voltar ao início do projeto e rodar novamente todas as nossas experimentações, usando-os como base e reparando no aumento de performance.
"""

empresas.write.parquet(
    path='/content/drive/MyDrive/Curso_Spark/empresas/parquet',
    mode='overwrite',
)

empresas_parquet = spark.read.parquet(
    '/content/drive/MyDrive/Curso_Spark/empresas/parquet'
)

empresas_parquet.printSchema()

"""## Particionamento dos dados

[DataFrameWriter.partitionBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html)

Entretanto, a depender do cenário, pode ser interessante criar um arquivo único - por exemplo, para enviá-lo a alguém ou por alguma outra necessidade do negócio. Isso é possível usando alguns recursos do Spark, ainda que seja uma operação que envolve bastante processamento (ou seja, que é considerada "cara").

Uma forma de fazermos isso é por meio do método coalesce() do Spark, demonstrado abaixo:
"""

empresas.coalesce(1).write.csv(
    path='/content/drive/MyDrive/Curso_Spark/empresas/csv-unico',
    mode='overwrite',
    sep=';',
    header=True
)

empresas.write.parquet(
    path='/content/drive/MyDrive/Curso_Spark/empresas/parquet-partitionBy',
    mode='overwrite',
        partitionBy='porte_da_empresa'
)

spark.stop()

